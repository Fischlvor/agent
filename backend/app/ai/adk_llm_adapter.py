"""
ADK LLM é€‚é…å™¨

å°†æˆ‘ä»¬çš„ LLM å®¢æˆ·ç«¯ï¼ˆQwenClient, OpenAIClient ç­‰ï¼‰é€‚é…åˆ° ADK çš„ BaseLlm æŽ¥å£
"""

import logging
from datetime import datetime
from typing import AsyncGenerator, Dict, Any, List, Optional
from google.adk.models import BaseLlm
from google.adk.models import LlmRequest, LlmResponse
from google.genai.types import Content, Part, FunctionCall, GenerateContentResponseUsageMetadata
from app.ai.clients.base import BaseAIClient

LOGGER = logging.getLogger(__name__)


class ADKLlmAdapter(BaseLlm):
    """
    ADK LLM é€‚é…å™¨

    å°†æˆ‘ä»¬çš„ BaseAIClient é€‚é…åˆ° ADK çš„ BaseLlm æŽ¥å£
    """

    # Pydantic é…ç½®ï¼šå…è®¸é¢å¤–å­—æ®µå’Œä»»æ„ç±»åž‹
    model_config = {"arbitrary_types_allowed": True, "extra": "allow"}

    def __init__(self, our_client: BaseAIClient, model_name: str = "custom", **kwargs):
        """
        åˆå§‹åŒ–é€‚é…å™¨

        Args:
            our_client: æˆ‘ä»¬çš„ LLM å®¢æˆ·ç«¯ï¼ˆQwenClient, OpenAIClient ç­‰ï¼‰
            model_name: æ¨¡åž‹åç§°ï¼ˆç”¨äºŽæ ‡è¯†ï¼‰
        """
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(model=model_name, **kwargs)

        # é€šè¿‡ __dict__ ç›´æŽ¥è®¾ç½®å­—æ®µï¼Œç»•è¿‡ Pydantic éªŒè¯
        object.__setattr__(self, 'our_client', our_client)
        object.__setattr__(self, 'model_name', model_name)

    async def generate_content_async(
        self,
        llm_request: LlmRequest,
        stream: bool = True,  # âœ… é»˜è®¤å¯ç”¨æµå¼
        **kwargs  # âœ… æŽ¥å—å…¶ä»–å¯èƒ½çš„å‚æ•°
    ) -> AsyncGenerator[LlmResponse, None]:
        """
        ç”Ÿæˆå†…å®¹ï¼ˆADK è¦æ±‚çš„æŽ¥å£ï¼‰

        âœ… çœŸæ­£çš„æµå¼ä¼ è¾“ï¼š
        1. è°ƒç”¨ chat_stream èŽ·å–å¢žé‡å“åº”
        2. ç´¯ç§¯å†…å®¹å¹¶é€æ­¥è¿”å›ž
        3. æ¯æ¬¡ yield ä¸€ä¸ª LlmResponseï¼ˆåŒ…å«åˆ°ç›®å‰ä¸ºæ­¢çš„æ‰€æœ‰å†…å®¹ï¼‰
        """
        # ============ æ­¥éª¤ 1ï¼šè½¬æ¢è¯·æ±‚æ ¼å¼ ============
        our_messages = self._convert_request_to_our_format(llm_request)

        # æ·»åŠ åŽ†å²æ¶ˆæ¯ï¼ˆä»Ž agent_adapter ä¼ æ¥çš„å®Œæ•´åŽ†å²ï¼‰
        if hasattr(self, 'agent_adapter') and hasattr(self.agent_adapter, '_history_messages'):
            history = self.agent_adapter._history_messages
            if history:
                our_messages = history + our_messages

        # æå–å·¥å…·å®šä¹‰
        our_tools = self._extract_tools_from_request(llm_request)

        # ============ æ­¥éª¤ 2ï¼šè°ƒç”¨æµå¼å®¢æˆ·ç«¯ ============
        accumulated_content = ""
        usage_metadata = None  # ç”¨äºŽä¿å­˜ token ç»Ÿè®¡ä¿¡æ¯

        response = await self.our_client.chat(
            messages=our_messages,
            system_prompt=None,
            tools=our_tools,
            stream=True,
            **kwargs
        )

        # å¤„ç†æµå¼å“åº”
        has_tool_call = False
        llm_start_time = datetime.utcnow()  # è®°å½•LLMè°ƒç”¨å¼€å§‹æ—¶é—´
        collected_tool_calls = None  # æ”¶é›† tool_callsï¼Œåœ¨æµç»“æŸåŽå¤„ç†

        async for chunk in response:
            # âœ… æå– token ç»Ÿè®¡ä¿¡æ¯ï¼ˆæœ€åŽä¸€ä¸ª chunk åŒ…å«ï¼‰
            if chunk.get("done") and ("prompt_eval_count" in chunk or "eval_count" in chunk):
                prompt_count = chunk.get("prompt_eval_count", 0)
                completion_count = chunk.get("eval_count", 0)
                LOGGER.info(f"ðŸ” OllamaåŽŸå§‹Tokenç»Ÿè®¡ - prompt_eval_count: {prompt_count}, eval_count: {completion_count}, prompt_cache_hit: {chunk.get('prompt_eval_duration', 0) == 0}")
                usage_metadata = GenerateContentResponseUsageMetadata(
                    prompt_token_count=prompt_count,
                    candidates_token_count=completion_count,
                    total_token_count=(prompt_count + completion_count)
                )

                # âœ… ä¿å­˜ ModelInvocation è®°å½•åˆ°æ•°æ®åº“ï¼Œå¹¶èŽ·å–åºå·
                llm_sequence = self._save_model_invocation(
                    prompt_tokens=prompt_count,
                    completion_tokens=completion_count,
                    total_tokens=prompt_count + completion_count,
                    start_time=llm_start_time,
                    finish_reason="STOP"
                )
                # ä¿å­˜æœ€è¿‘çš„ LLM åºå·ï¼Œä¾›å·¥å…·è°ƒç”¨ä½¿ç”¨
                if llm_sequence is not None:
                    object.__setattr__(self, 'last_llm_sequence', llm_sequence)

            # âœ… Ollama æµå¼å“åº”æ ¼å¼ï¼š{"message": {"role": "assistant", "content": "..."}, "done": false}
            if "message" in chunk:
                message = chunk["message"]

                # æå–å¢žé‡å†…å®¹
                if "content" in message and message["content"]:
                    delta = message["content"]
                    accumulated_content += delta  # ä¿ç•™ç´¯ç§¯ç”¨äºŽå·¥å…·è°ƒç”¨åœºæ™¯

                    # âœ… è¿”å›žå¢žé‡å†…å®¹ï¼ˆdeltaï¼‰ï¼Œä¸æ˜¯ç´¯ç§¯å†…å®¹ï¼
                    adk_response = self._create_streaming_response(delta)
                    yield adk_response

                # âœ… æ”¶é›†å·¥å…·è°ƒç”¨ï¼ˆä¸ç«‹å³yieldï¼Œç­‰doneæ—¶å†å¤„ç†ï¼‰
                if "tool_calls" in message and message["tool_calls"]:
                    has_tool_call = True
                    collected_tool_calls = message["tool_calls"]

        # âœ… æµå¼ç»“æŸåŽå¤„ç†å·¥å…·è°ƒç”¨ï¼ˆæ­¤æ—¶ last_llm_sequence å·²è®¾ç½®ï¼‰
        if has_tool_call and collected_tool_calls:
            # è½¬æ¢å·¥å…·è°ƒç”¨ä¸º ADK æ ¼å¼
            function_calls = []
            for tool_call in collected_tool_calls:
                # Ollama æ ¼å¼: {"function": {"name": "...", "arguments": {...}}}
                function_info = tool_call.get("function", {})
                tool_name = function_info.get("name", "")

                function_call = FunctionCall(
                    name=tool_name,
                    args=function_info.get("arguments", {})
                )
                function_calls.append(function_call)

            # åˆ›å»ºåŒ…å«å·¥å…·è°ƒç”¨çš„å“åº”
            if function_calls:
                parts = []
                for fc in function_calls:
                    parts.append(Part(function_call=fc))

                response_content = Content(role="model", parts=parts)
                adk_response = LlmResponse(
                    content=response_content,
                    turn_complete=True,  # å·¥å…·è°ƒç”¨è¡¨ç¤ºä¸€è½®å®Œæˆ
                    finish_reason="STOP",
                    usage_metadata=usage_metadata  # âœ… æ·»åŠ  token ç»Ÿè®¡
                )
                yield adk_response

        # âœ… æµå¼ç»“æŸï¼šå¦‚æžœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¿”å›žä¸€ä¸ªå®Œæˆæ ‡è®°ï¼ˆåŒ…å« usage_metadataï¼‰
        elif accumulated_content and not has_tool_call:
            final_response = LlmResponse(
                content=Content(role="model", parts=[Part(text="")]),  # ç©ºå†…å®¹ï¼Œä»…ä½œä¸ºç»“æŸæ ‡è®°
                turn_complete=True,
                finish_reason="STOP",
                usage_metadata=usage_metadata  # âœ… æ·»åŠ  token ç»Ÿè®¡
            )
            yield final_response

        # ============ æ­¥éª¤ 3ï¼šå¦‚æžœæ²¡æœ‰æµå¼å†…å®¹ï¼Œè¿”å›žå®Œæ•´å“åº”ï¼ˆé™çº§ï¼‰ ============
        if not accumulated_content:
            our_response = await self.our_client.chat(
                messages=our_messages,
                system_prompt=None,
                tools=None,
                stream=False
            )
            adk_response = self._convert_response_to_adk_format(our_response)
            yield adk_response

    def _extract_tools_from_request(self, request: LlmRequest) -> Optional[List[Dict[str, Any]]]:
        """
        ä»Ž ADK LlmRequest æå–å·¥å…·å®šä¹‰

        ADK æ ¼å¼ï¼šrequest.config.tools = [Tool(function_declarations=[...])]
        Ollama æ ¼å¼ï¼š[{"type": "function", "function": {...}}]
        """
        if not hasattr(request, 'config') or not request.config:
            return None

        if not hasattr(request.config, 'tools') or not request.config.tools:
            return None

        our_tools = []

        for tool in request.config.tools:
            if hasattr(tool, 'function_declarations'):
                for func_decl in tool.function_declarations:
                    # è½¬æ¢ Schema ä¸º JSON Schema
                    parameters = self._convert_schema_to_json(func_decl.parameters) if hasattr(func_decl, 'parameters') else {                    }

                    our_tools.append({
                        "type": "function",
                        "function": {
                            "name": func_decl.name,
                            "description": func_decl.description or "",
                            "parameters": parameters
                        }
                    })

        return our_tools if our_tools else None

    def _convert_schema_to_json(self, schema: Any) -> Dict[str, Any]:
        """
        è½¬æ¢ ADK Schema ä¸º JSON Schema

        ADK Schema.type æ˜¯æžšä¸¾ï¼Œéœ€è¦è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        """
        if not schema:
            return {}

        result = {}

        # Type
        if hasattr(schema, 'type'):
            result['type'] = schema.type.name.lower() if hasattr(schema.type, 'name') else str(schema.type)

        # Properties
        if hasattr(schema, 'properties') and schema.properties:
            result['properties'] = {}
            for key, prop_schema in schema.properties.items():
                result['properties'][key] = self._convert_schema_to_json(prop_schema)

        # Required
        if hasattr(schema, 'required') and schema.required:
            result['required'] = list(schema.required)

        # Description
        if hasattr(schema, 'description') and schema.description:
            result['description'] = schema.description

        return result

    def _convert_request_to_our_format(self, request: LlmRequest) -> list:
        """
        è½¬æ¢ ADK LlmRequest åˆ°æˆ‘ä»¬çš„æ¶ˆæ¯æ ¼å¼

        ADK request.contents æ˜¯ List[Content]
        æˆ‘ä»¬çš„æ ¼å¼æ˜¯ List[Dict[str, str]]

        âœ… éœ€è¦å¤„ç†ï¼š
        1. æ–‡æœ¬å†…å®¹ï¼ˆPart.textï¼‰
        2. å·¥å…·è°ƒç”¨ï¼ˆPart.function_callï¼‰
        3. å·¥å…·ç»“æžœï¼ˆPart.function_response æˆ– role="tool"ï¼‰
        """
        our_messages = []

        # ADK çš„ Content å¯¹è±¡åŒ…å« role å’Œ parts
        for content in request.contents:
            role = content.role if hasattr(content, 'role') else 'user'

            # âœ… å¤„ç†ä¸åŒç±»åž‹çš„ parts
            if not hasattr(content, 'parts'):
                continue

            message_data = {"role": role}
            text_parts = []
            tool_calls = []
            tool_result = None

            for part in content.parts:
                # æ–‡æœ¬å†…å®¹
                if hasattr(part, 'text') and part.text:
                    text_parts.append(part.text)

                # å·¥å…·è°ƒç”¨ï¼ˆAssistant æ¶ˆæ¯ï¼‰
                elif hasattr(part, 'function_call') and part.function_call:
                    import json
                    fc = part.function_call
                    # âœ… Ollama æœŸæœ› arguments æ˜¯ JSON å­—ç¬¦ä¸²ï¼ˆæˆ–å­—å…¸ï¼Œå–å†³äºŽå®žçŽ°ï¼‰
                    args = fc.args if isinstance(fc.args, dict) else {}
                    tool_calls.append({
                        "id": f"call_{len(tool_calls)}",
                        "type": "function",
                        "function": {
                            "name": fc.name,
                            "arguments": args  # ä¿æŒå­—å…¸æ ¼å¼
                        }
                    })

                # å·¥å…·ç»“æžœï¼ˆTool æ¶ˆæ¯ï¼‰
                elif hasattr(part, 'function_response') and part.function_response:
                    fr = part.function_response
                    # âœ… æå–å­—ç¬¦ä¸²å½¢å¼çš„ç»“æžœ
                    result_str = fr.response if hasattr(fr, 'response') else str(fr)
                    # å¦‚æžœæ˜¯å­—å…¸ï¼Œè½¬æ¢ä¸º JSON å­—ç¬¦ä¸²
                    if isinstance(result_str, dict):
                        import json
                        result_str = json.dumps(result_str, ensure_ascii=False)
                    tool_result = {
                        "name": fr.name if hasattr(fr, 'name') else "",
                        "content": result_str
                    }

            # æž„é€ æ¶ˆæ¯
            if tool_calls:
                # Assistant æ¶ˆæ¯ + å·¥å…·è°ƒç”¨
                message_data["content"] = ' '.join(text_parts) if text_parts else ""
                message_data["tool_calls"] = tool_calls
            elif tool_result:
                # Tool ç»“æžœæ¶ˆæ¯
                message_data["role"] = "tool"
                message_data["content"] = tool_result["content"]
                message_data["tool_call_id"] = "call_0"  # Ollama éœ€è¦è¿™ä¸ªå­—æ®µ
            else:
                # æ™®é€šæ–‡æœ¬æ¶ˆæ¯
                message_data["content"] = ' '.join(text_parts)

            our_messages.append(message_data)

        # âœ… åˆå¹¶è¿žç»­çš„åŒè§’è‰²æ–‡æœ¬æ¶ˆæ¯ï¼ˆé¿å…æµå¼ chunks äº§ç”Ÿå¤§é‡é‡å¤æ¶ˆæ¯ï¼‰
        merged_messages = []
        for msg in our_messages:
            # å¦‚æžœæ˜¯å¸¦å·¥å…·çš„æ¶ˆæ¯ï¼Œç›´æŽ¥æ·»åŠ 
            if "tool_calls" in msg or msg.get("role") == "tool":
                merged_messages.append(msg)
                continue

            # å°è¯•ä¸Žå‰ä¸€æ¡æ¶ˆæ¯åˆå¹¶
            if merged_messages and merged_messages[-1].get("role") == msg.get("role") \
                    and "tool_calls" not in merged_messages[-1]:
                # åˆå¹¶æ–‡æœ¬å†…å®¹
                merged_messages[-1]["content"] += msg.get("content", "")
            else:
                # æ–°æ¶ˆæ¯
                merged_messages.append(msg)

        return merged_messages

    def _create_streaming_response(self, content: str) -> LlmResponse:
        """
        åˆ›å»ºæµå¼å“åº”ï¼ˆADK LlmResponse æ ¼å¼ï¼‰

        Args:
            content: ç´¯ç§¯çš„æ–‡æœ¬å†…å®¹

        Returns:
            LlmResponse å¯¹è±¡ï¼ˆæµå¼ï¼Œturn_complete=Falseï¼‰
        """
        response_content = Content(
            role="model",
            parts=[Part(text=content)]
        )

        return LlmResponse(
            content=response_content,
            turn_complete=False,  # âœ… æµå¼ä¸­ï¼Œæ¯ä¸ªå—éƒ½æ˜¯ incomplete
            finish_reason=None
        )

    def _convert_response_to_adk_format(self, our_response: dict) -> LlmResponse:
        """
        è½¬æ¢æˆ‘ä»¬çš„å“åº”åˆ° ADK LlmResponse æ ¼å¼

        æˆ‘ä»¬çš„å“åº”ï¼š{"content": "...", "tool_calls": [...]}
        ADK å“åº”ï¼šLlmResponse å¯¹è±¡
        """
        # åˆ›å»º ADK çš„ Content å¯¹è±¡
        content_parts = []

        # æ·»åŠ æ–‡æœ¬å†…å®¹
        if "content" in our_response and our_response["content"]:
            content_parts.append(Part(text=our_response["content"]))

        # æ·»åŠ å·¥å…·è°ƒç”¨ï¼ˆå¦‚æžœæœ‰ï¼‰
        if "tool_calls" in our_response and our_response["tool_calls"]:
            # âœ… è½¬æ¢å·¥å…·è°ƒç”¨æ ¼å¼åˆ° ADK FunctionCall
            for tool_call in our_response["tool_calls"]:
                # Ollama æ ¼å¼: {"function": {"name": "...", "arguments": {...}}}
                function_info = tool_call.get("function", {})
                function_call = FunctionCall(
                    name=function_info.get("name", ""),
                    args=function_info.get("arguments", {})
                )
                content_parts.append(Part(function_call=function_call))

        # åˆ›å»º Content å¯¹è±¡
        response_content = Content(
            role="model",
            parts=content_parts
        )

        # åˆ›å»º LlmResponse
        # âœ… ä½¿ç”¨æ­£ç¡®çš„å­—æ®µç»“æž„
        adk_response = LlmResponse(
            content=response_content,  # âœ… Content å¯¹è±¡
            turn_complete=True,  # âœ… è¡¨ç¤ºè¿™ä¸€è½®å¯¹è¯å®Œæˆ
            finish_reason="STOP"  # âœ… å®ŒæˆåŽŸå› ï¼ˆADK ä¼šé€šè¿‡ parts æ£€æµ‹ FunctionCallï¼‰
        )

        return adk_response

    def _save_model_invocation(
        self,
        prompt_tokens: int,
        completion_tokens: int,
        total_tokens: int,
        start_time: datetime,
        finish_reason: Optional[str] = None
    ) -> Optional[int]:
        """
        ä¿å­˜ ModelInvocation è®°å½•åˆ°æ•°æ®åº“

        Args:
            prompt_tokens: è¾“å…¥tokenæ•°
            completion_tokens: è¾“å‡ºtokenæ•°
            total_tokens: æ€»tokenæ•°
            start_time: å¼€å§‹æ—¶é—´
            finish_reason: å®ŒæˆåŽŸå› 

        Returns:
            ä¿å­˜çš„åºå·ï¼Œå¦‚æžœä¿å­˜å¤±è´¥åˆ™è¿”å›ž None
        """
        # æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦çš„ä¿¡æ¯
        if not hasattr(self, 'db_session') or not self.db_session:
            LOGGER.debug("æ²¡æœ‰ db_sessionï¼Œè·³è¿‡ä¿å­˜ ModelInvocation")
            return None

        if not hasattr(self, 'current_message_id') or not self.current_message_id:
            LOGGER.debug("æ²¡æœ‰ current_message_idï¼Œè·³è¿‡ä¿å­˜ ModelInvocation")
            return None

        if not hasattr(self, 'current_session_id') or not self.current_session_id:
            LOGGER.debug("æ²¡æœ‰ current_session_idï¼Œè·³è¿‡ä¿å­˜ ModelInvocation")
            return None

        try:
            # å¯¼å…¥ ModelInvocationï¼ˆå»¶è¿Ÿå¯¼å…¥é¿å…å¾ªçŽ¯ä¾èµ–ï¼‰
            from app.models.invocation import ModelInvocation

            # é€’å¢žåºå·
            if not hasattr(self, 'llm_sequence_counter'):
                object.__setattr__(self, 'llm_sequence_counter', 0)

            current_sequence = self.llm_sequence_counter + 1
            object.__setattr__(self, 'llm_sequence_counter', current_sequence)

            # è®¡ç®—è€—æ—¶
            duration_ms = int((datetime.utcnow() - start_time).total_seconds() * 1000)

            # åˆ›å»ºè®°å½•
            invocation = ModelInvocation(
                message_id=str(self.current_message_id),
                session_id=str(self.current_session_id),
                sequence_number=current_sequence,
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens,
                total_tokens=total_tokens,
                duration_ms=duration_ms,
                model_name=self.model_name,
                finish_reason=finish_reason,
                created_at=datetime.utcnow()
            )

            # ä¿å­˜åˆ°æ•°æ®åº“
            self.db_session.add(invocation)
            self.db_session.flush()  # ç«‹å³å†™å…¥ï¼Œä½†ä¸æäº¤ï¼ˆç”±å¤–å±‚æŽ§åˆ¶æäº¤ï¼‰

            LOGGER.info(
                f"âœ… å·²ä¿å­˜ ModelInvocation è®°å½• #{current_sequence}: "
                f"tokens={total_tokens}, duration={duration_ms}ms"
            )

            return current_sequence

        except Exception as e:
            LOGGER.error(f"ä¿å­˜ ModelInvocation è®°å½•å¤±è´¥: {e}", exc_info=True)
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…å½±å“æ­£å¸¸æµç¨‹
            return None

    def connect(self):
        """
        è¿žæŽ¥åˆ° LLM æœåŠ¡ï¼ˆå¦‚æžœéœ€è¦ï¼‰

        æˆ‘ä»¬çš„å®¢æˆ·ç«¯é€šå¸¸åœ¨åˆå§‹åŒ–æ—¶å°±è¿žæŽ¥äº†ï¼Œæ‰€ä»¥è¿™é‡Œå¯ä»¥ä¸ºç©º
        """
        return None

    @classmethod
    def supported_models(cls) -> list[str]:
        """
        è¿”å›žæ”¯æŒçš„æ¨¡åž‹åˆ—è¡¨

        è¿™ä¸ªæ–¹æ³•æ˜¯ BaseLlm è¦æ±‚çš„
        """
        return [
            "qwen:8b",
            "qwen:14b",
            "qwen3:8b",
            "openai:gpt-4",
            "openai:gpt-3.5-turbo"
        ]


# ============ å·¥åŽ‚å‡½æ•° ============
def create_adk_llm_from_our_client(our_client: BaseAIClient, model_name: str) -> ADKLlmAdapter:
    """
    å·¥åŽ‚å‡½æ•°ï¼šä»Žæˆ‘ä»¬çš„å®¢æˆ·ç«¯åˆ›å»º ADK LLM é€‚é…å™¨

    Args:
        our_client: æˆ‘ä»¬çš„ BaseAIClient å®žä¾‹
        model_name: æ¨¡åž‹åç§°

    Returns:
        ADKLlmAdapter å®žä¾‹

    Example:
        >>> from app.ai.clients.qwen_client import QwenClient
        >>> from app.ai.factory import FACTORY
        >>>
        >>> our_client = FACTORY.create_client("ollama", "qwen3:8b", "http://localhost:11434")
        >>> adk_llm = create_adk_llm_from_our_client(our_client, "qwen3:8b")
        >>>
        >>> # çŽ°åœ¨å¯ä»¥ä¼ ç»™ ADK Agent
        >>> from google.adk import Agent
        >>> agent = Agent(name="test", model=adk_llm)
    """
    return ADKLlmAdapter(our_client=our_client, model_name=model_name)

