"""æ£€ç´¢æœåŠ¡ï¼ˆå¬å›+é‡æ’+å»é‡ï¼‰- åŸºäº PGVector"""

import time
from typing import List, Dict, Any
from sqlalchemy.orm import Session
from sqlalchemy import and_, text
import logging

from app.models.rag import Document, DocumentChunk, ChunkType
from app.services.rag.model_manager import get_embeddings, get_reranker
from app.schemas.rag import RetrievalResult, MatchedChild

logger = logging.getLogger(__name__)


class RetrievalService:
    """æ£€ç´¢æœåŠ¡ - åŸºäº PGVector å‘é‡æ£€ç´¢"""

    def __init__(self, db: Session):
        self.db = db
        # ä½¿ç”¨å…¨å±€å•ä¾‹çš„æ¨¡å‹ï¼ˆä¸ä¼šé‡å¤åŠ è½½ï¼‰
        self.embeddings = get_embeddings()
        self.reranker = get_reranker()

    async def retrieve(
        self,
        query: str,
        kb_id: int,
        top_k: int = 5,
        top_k_recall: int = 20,
        similarity_threshold: float = 0.7,
        use_rerank: bool = True
    ) -> tuple[List[RetrievalResult], int]:
        """
        ä¸¤é˜¶æ®µæ£€ç´¢ï¼šå¬å› + é‡æ’ + å»é‡

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            kb_id: çŸ¥è¯†åº“ID
            top_k: æœ€ç»ˆè¿”å›çš„çˆ¶å—æ•°é‡
            top_k_recall: å¬å›çš„å­å—æ•°é‡
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼Œä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
            use_rerank: æ˜¯å¦ä½¿ç”¨é‡æ’åº

        Returns:
            (results, search_time_ms)
        """
        start_time = time.time()

        # ===== é˜¶æ®µ 1ï¼šå‘é‡å¬å›ï¼ˆä½¿ç”¨ pgvectorï¼‰=====
        logger.info(f"å¼€å§‹æ£€ç´¢: query='{query[:50]}...', kb_id={kb_id}")

        # 1.1 å°†æŸ¥è¯¢æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡
        query_embedding = self.embeddings.embed_query(query)
        logger.info(f"ğŸ” query_embeddingç±»å‹={type(query_embedding)}, ç»´åº¦={len(query_embedding)}, å‰5å€¼={query_embedding[:5]}")

        # 1.2 ä½¿ç”¨ pgvector çš„ä½™å¼¦ç›¸ä¼¼åº¦æœç´¢ï¼ˆ1 - ä½™å¼¦è·ç¦» = ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        # pgvector çš„ <=> æ“ä½œç¬¦è®¡ç®—ä½™å¼¦è·ç¦»ï¼Œéœ€è¦è½¬æ¢ä¸ºç›¸ä¼¼åº¦
        sql = text("""
            SELECT
                dc.id,
                dc.parent_id,
                dc.content,
                dc.chunk_index,
                dc.page_number,
                dc.section_title,
                dc.metadata,
                1 - (dc.embedding <=> :query_embedding) AS similarity
            FROM document_chunks dc
            JOIN documents d ON dc.doc_id = d.id
            WHERE d.kb_id = :kb_id
                AND dc.chunk_type = 'CHILD'
                AND dc.embedding IS NOT NULL
                AND 1 - (dc.embedding <=> :query_embedding) >= :threshold
            ORDER BY dc.embedding <=> :query_embedding
            LIMIT :limit
        """)

        query_embedding_str = str(query_embedding)
        logger.info(f"ğŸ” query_embeddingå­—ç¬¦ä¸²æ ¼å¼: {query_embedding_str[:100]}...")
        logger.info(f"ğŸ” æŸ¥è¯¢å‚æ•°: kb_id={kb_id}, threshold={similarity_threshold}, limit={top_k_recall}")

        result = self.db.execute(
            sql,
            {
                "query_embedding": query_embedding_str,  # pgvector æ¥å—å­—ç¬¦ä¸²æ ¼å¼çš„å‘é‡
                "kb_id": kb_id,
                "threshold": similarity_threshold,
                "limit": top_k_recall
            }
        )

        child_results = result.fetchall()

        if not child_results:
            logger.warning(f"âŒ æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ (kb_id={kb_id}, threshold={similarity_threshold})")
            return [], int((time.time() - start_time) * 1000)

        logger.info(f"å¬å› {len(child_results)} ä¸ªå­å—")

        # æ„é€ å­å—æ•°æ®
        child_ids = [row[0] for row in child_results]
        child_scores = {row[0]: float(row[7]) for row in child_results}

        # ===== é˜¶æ®µ 2ï¼šé‡æ’åºï¼ˆå¯é€‰ï¼‰=====
        if use_rerank and len(child_results) > 1:
            chunk_texts = [row[2] for row in child_results]  # content
            rerank_scores = await self.reranker.rerank(query, chunk_texts)

            logger.info(f"é‡æ’åºå®Œæˆ")

            # æ›´æ–°åˆ†æ•°
            for cid, rerank_score in zip(child_ids, rerank_scores):
                child_scores[cid] = rerank_score

        # ===== é˜¶æ®µ 3ï¼šèšåˆçˆ¶å— + å»é‡ =====
        parent_groups = {}

        for row in child_results:
            child_id, parent_id, content, chunk_index, page_number, section_title, metadata, similarity = row

            if parent_id is None:
                logger.warning(f"å­å— {child_id} æ²¡æœ‰çˆ¶å—")
                continue

            if parent_id not in parent_groups:
                parent_groups[parent_id] = {
                    'children': [],
                    'max_score': 0.0
                }

            score = child_scores.get(child_id, 0.0)
            parent_groups[parent_id]['children'].append({
                'child_id': child_id,
                'content': content,
                'chunk_index': chunk_index,
                'page_number': page_number,
                'score': score
            })
            parent_groups[parent_id]['max_score'] = max(
                parent_groups[parent_id]['max_score'],
                score
            )

        logger.info(f"èšåˆåˆ° {len(parent_groups)} ä¸ªçˆ¶å—")

        # ===== é˜¶æ®µ 4ï¼šæŒ‰æœ€é«˜åˆ†æ•°æ’åº + å– Top-K =====
        sorted_parents = sorted(
            parent_groups.items(),
            key=lambda x: x[1]['max_score'],
            reverse=True
        )[:top_k]

        # ===== é˜¶æ®µ 5ï¼šæ„é€ è¿”å›ç»“æœ =====
        results = []
        parent_ids = [pid for pid, _ in sorted_parents]

        # æ‰¹é‡æŸ¥è¯¢çˆ¶å—
        parents = self.db.query(DocumentChunk).filter(
            and_(
                DocumentChunk.id.in_(parent_ids),
                DocumentChunk.chunk_type == ChunkType.PARENT
            )
        ).all()
        parent_dict = {p.id: p for p in parents}

        # æ‰¹é‡æŸ¥è¯¢æ–‡æ¡£
        doc_ids = list(set(p.doc_id for p in parents))
        documents = self.db.query(Document).filter(
            Document.id.in_(doc_ids)
        ).all()
        doc_dict = {d.id: d for d in documents}

        for pid, group_data in sorted_parents:
            if pid not in parent_dict:
                continue

            parent = parent_dict[pid]
            doc = doc_dict.get(parent.doc_id)

            # æ„é€ åŒ¹é…çš„å­å—åˆ—è¡¨
            matched_children = []
            for child_data in group_data['children']:
                matched_children.append(MatchedChild(
                    child_id=child_data['child_id'],
                    child_text=child_data['content'],
                    score=child_data['score'],
                    chunk_index=child_data['chunk_index']
                ))

            # æŒ‰åˆ†æ•°æ’åºå­å—
            matched_children.sort(key=lambda x: x.score, reverse=True)

            result = RetrievalResult(
                parent_id=parent.id,
                parent_text=parent.content,
                doc_id=parent.doc_id,
                doc_title=doc.metadata_.get('title') if doc and doc.metadata_ else None,
                section=parent.section_title,
                page_number=parent.page_number,
                matched_children=matched_children,
                max_score=group_data['max_score'],
                source=doc.filename if doc else "Unknown"
            )
            results.append(result)

        search_time_ms = int((time.time() - start_time) * 1000)
        logger.info(f"æ£€ç´¢å®Œæˆ: è¿”å› {len(results)} ä¸ªç»“æœ, è€—æ—¶ {search_time_ms}ms")

        return results, search_time_ms
